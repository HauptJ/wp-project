---
layout: page
title: "S3 and nginx image proxy"
category: uploads
date: 2017-03-17 10:29:30
order: 8
---

## Introduction

This page shows you how to store the uploaded files eg. images in AWS S3. Uploads should never be stored locally in production because we want to keep the WordPress docker containers as stateless as possible.

You will also learn how to use nginx with pagespeed module to serve, cache and rewrite images stored in s3.


## Install terraform

We have created terraform script can be used to create the bucket and credentials. In order to use terraform you need to install it:

```bash
$ brew install terraform
```

## Retrieve your AWS access key and secret key

To use terraform you need to have permissions to create new users/policies/buckets. Retrieve your access key and secret key from AWS admin console. This guide can be helpful:

http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey

## Create the new s3 bucket using terraform

You need set a name for the bucket. The name can be anything but it needs to be unique. In this example it is `geniem-media`.

The script creates 2 buckets `geniem-media` in `eu-west-1` region and replica `geniem-media-replica-1` in `eu-central-1` region.

The script will output an access key and a secret key. In this example they are `XXXXXXXXXXXXXXXXXXXX` and `YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY` remember to replace them.

```bash
$ terraform apply github.com/devgeniem/terraform/s3-bucket-for-uploads
```

## Setup nginx proxy for the images

Use your new s3 bucket as upstream in nginx: `nginx/http/image-proxy.conf.tmpl` ->

```nginx
# Use AWS s3 bucket with replication to
upstream aws_s3_bucket {
	# This is the primary s3 zone
    server ${S3_BUCKET_NAME}.s3-eu-west-1.amazonaws.com:443;

    # This is alternative bucket which is automatically replicated from the main bucket
    server ${S3_BUCKET_NAME}-replica-1.s3-eu-central-1.amazonaws.com:443 backup;

    # Use keepalive for faster connections
    keepalive 2;
}

# Use cache for images from Google Bucket
proxy_cache_path /tmp/nginx/images/ levels=1:2 keys_zone=aws_s3_cache:100m max_size=512m inactive=168h use_temp_path=off;
```

Add new location for nginx to serve images from `/uploads/` path: `nginx/server/image-proxy.conf.tmpl` ->

```nginx
##
# Proxy all uploads from S3 bucket and allow ngx_pagespeed to optimize them
##
location ~* ^/uploads/ {
  proxy_set_header        Accept-Encoding "";
  proxy_http_version      1.1;

  # Remove cookies and don't send them to the end clients
  proxy_hide_header       Set-Cookie;
  proxy_ignore_headers    "Set-Cookie";

  # Hide s3 bucket extra headers
  proxy_hide_header       x-amz-id-2;
  proxy_hide_header       x-amz-request-id;
  proxy_hide_header       Alt-Svc;

  proxy_buffering         off;

  # Try different upstreams if they fail
  proxy_intercept_errors  on;

  # Use proxy cache
  proxy_cache aws_s3_cache;

  # Set long cache headers
  expires max;
  add_header Cache-Control "public, max-age=31536000";

  # Allow pagespeed to optimize images from this cache
  pagespeed AllowVaryOn Auto;

  # Resolve domain name to AWS s3 bucket
  resolver               8.8.8.8 valid=300s;
  resolver_timeout       10s;

  # Cache files for one week, cache 404 files for a moment to reduce DDOS just a little bit
  proxy_cache_valid 200 168h;
  proxy_cache_valid 404 10s;

  # Try next upstream if the main upstream is down
  proxy_next_upstream error timeout http_404;

  proxy_pass             https://aws_s3_bucket;
}
```

## Define s3 uploads settings to WordPress

Recommendation is to use different bucket with different environments in `config/environments/{development,staging,production}.php` ->

```
/**
 * Store uploads in S3
 */
define( 'S3_UPLOADS_BUCKET',    'geniem-media' );
define( 'S3_UPLOADS_KEY',       'XXXXXXXXXXXXXXXXXXXX' );
define( 'S3_UPLOADS_SECRET',    'YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY' );
define( 'S3_UPLOADS_REGION',    'eu-west-1' );
```