---
layout: page
title: "S3 and nginx image proxy"
category: uploads
date: 2017-03-17 10:29:30
order: 8
---

## Introduction

This page shows you how to store the uploaded files eg. images in AWS S3. Uploads should never be stored locally in production because we want to keep the WordPress docker containers as stateless as possible.

You will also learn how to use nginx with pagespeed module to serve, cache and rewrite images stored in s3.


## Install terraform

We have created terraform script can be used to create the bucket and credentials. In order to use terraform you need to install it:

```bash
$ brew install terraform
```

## Retrieve your AWS access key and secret key

To use terraform you need to have permissions to create new users/policies/buckets. Retrieve your access key and secret key from AWS admin console. This guide can be helpful:

[http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey)

## Create the new s3 bucket using terraform

You need set a name for the bucket. The name can be anything but it needs to be unique. In this example it is `geniem-media`.

The script creates 2 buckets `geniem-media` in `eu-west-1` region and replica `geniem-media-replica-1` in `eu-central-1` region.

The script will output an access key and a secret key. In this example they are `XXXXXXXXXXXXXXXXXXXX` and `YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY` remember to replace them with real ones in WordPress options in later steps.

```bash
$ terraform apply github.com/devgeniem/terraform/s3-bucket-for-uploads
var.aws_bucket_name
  Enter a value: geniem-media

...

Apply complete! Resources: 8 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the `terraform show` command.

State path: terraform.tfstate

Outputs:

s3-bucket-name-main = geniem-media
s3-user-access-key = XXXXXXXXXXXXXXXXXXXX
s3-user-secret-key = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
```

## Setup nginx proxy for the images

**Add these 2 files in nginx configs to serve images from s3.**

**NOTE: Remember to rename bucket example name `geniem-media` with the bucket that you used**

Use your new s3 bucket as upstream in nginx: `nginx/http/image-proxy.conf.tmpl` ->

```nginx
# Use AWS s3 bucket with replication to
upstream aws_s3_bucket {
    # This is the primary s3 zone
    server geniem-media.s3-eu-west-1.amazonaws.com:443;

    # This is alternative bucket which is automatically replicated from the main bucket
    server geniem-media-replica-1.s3-eu-central-1.amazonaws.com:443 backup;

    # Use keepalive for faster connections
    keepalive 2;
}

# Use cache for images from Google Bucket
proxy_cache_path /tmp/nginx/images/ levels=1:2 keys_zone=aws_s3_cache:100m max_size=512m inactive=168h use_temp_path=off;
```

Add new location for nginx to serve images from `/uploads/` path: `nginx/server/image-proxy.conf.tmpl` ->

```nginx
##
# Proxy all uploads from S3 bucket and allow ngx_pagespeed to optimize them
##
location ~* ^/uploads/ {
  proxy_set_header        Accept-Encoding "";
  proxy_http_version      1.1;

  # Remove cookies and don't send them to the end clients
  proxy_hide_header       Set-Cookie;
  proxy_ignore_headers    "Set-Cookie";

  # Hide s3 bucket extra headers
  proxy_hide_header       x-amz-id-2;
  proxy_hide_header       x-amz-request-id;
  proxy_hide_header       Alt-Svc;

  proxy_buffering         off;

  # Try different upstreams if they fail
  proxy_intercept_errors  on;

  # Use proxy cache
  proxy_cache aws_s3_cache;

  # Set long cache headers
  expires max;
  add_header Cache-Control "public, max-age=31536000";

  # Allow pagespeed to optimize images from this cache
  pagespeed AllowVaryOn Auto;

  # Resolve domain name to AWS s3 bucket
  resolver               8.8.8.8 valid=300s;
  resolver_timeout       10s;

  # Cache files for one week, cache 404 files for a moment to reduce DDOS just a little bit
  proxy_cache_valid 200 168h;
  proxy_cache_valid 404 10s;

  # Try next upstream if the main upstream is down
  proxy_next_upstream error timeout http_404;

  proxy_pass             https://aws_s3_bucket;
}
```

## Install humanmade/s3-uploads

To enable s3 uploads we need to install [humanmade/S3-uploads](https://github.com/humanmade/S3-Uploads)
plugin. Go to your WordPress project folder and run:

```bash
$ composer require humanmade/s3-uploads
```

## Define s3 uploads settings to WordPress

Recommendation is to use different bucket with different environments in `config/environments/{development,staging,production}.php` ->

```php
/**
 * Store uploads in S3
 */

// Set credentials
define( 'S3_UPLOADS_BUCKET',    'geniem-media' );
define( 'S3_UPLOADS_KEY',       'XXXXXXXXXXXXXXXXXXXX' );
define( 'S3_UPLOADS_SECRET',    'YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY' );
define( 'S3_UPLOADS_REGION',    'eu-west-1' );

// Expire in 1 year
define( 'S3_UPLOADS_HTTP_CACHE_CONTROL', 365 * 24 * 60 * 60  );
define( 'S3_UPLOADS_HTTP_EXPIRES', gmdate( 'D, d M Y H:i:s', time() + (365 * 24 * 60 * 60) ) .' GMT' );

// Rewrite all assets into this SITEURL where /uploads/ is handled with nginx+pagespeed
define( 'S3_UPLOADS_BUCKET_URL', WP_SITEURL  );
```

## Activate the s3 plugin

Final step is to activate the s3 uploads plugin

```bash
$ wp plugin activate s3-uploads
```
```